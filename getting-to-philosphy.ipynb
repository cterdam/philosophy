{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A funny thing many people have noticed about Wikipedia is \n",
    "A funny thing that I found in my explorations is that if I go from page to page by clicking the first link in each article, I eventually find myself at the philosophy article. I wonder if that will happen every time. I'm going to try it out. I'll click on the first link on each Wikipedia page I come to. So, here's what I'm doing. Let me start at some Wikipedia page like the article for African swallow. I suppose I could have used the random article feature if I didn't have this topic in mind already. Now, I'm going to click the first link I come to in the main part of the article. I'll skip the disambiguation link and head to the article proper. That takes me to the article for bird. The first link here is endothermic. So, I can keep going on like this for quite a long time. The first link in this article takes me to ancient Greek, and the next step is Greek language. From there, I get to modern Greek. Here, the first link is a pronunciation help link. I think it's a good idea to skip over that. It's not taking me to another article, just teaching me how to say modern Greek in modern Greek. The first link to another article is colloquially. After that comes word, and then linguistics, and then scientific. These articles are getting more and more abstract. The first link in this article is knowledge, and that leads to awareness, and then quality which is a philosophical term. So eventually, I make it to Philosophy. So, the process is go to a Wikipedia page and find the first ordinary link in the main part of the text. Click through to the new page and repeat the process. Keep going until you reach philosophy, which seems to happen pretty often or until you get tired of clicking. Let's try the process again. Hmm, where should I start? Why not the chair article? The first link in the article is here, the word furniture. And the first link in the furniture article takes me back to chair. So not every article chain makes it to philosophy. Instead, some chains make loops. We can try this with some other articles to see what happens. All this clicking is a bit slow though. This is something we can automate with Python. It's surprising how often I find that everyday tasks can be made better with programming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Special:Random\n",
      "https://en.wikipedia.org/wiki/Ahmad_Jamal\n",
      "https://en.wikipedia.org/wiki/Jazz\n",
      "https://en.wikipedia.org/wiki/Music_genre\n",
      "https://en.wikipedia.org/wiki/Music\n",
      "https://en.wikipedia.org/wiki/Art\n",
      "https://en.wikipedia.org/wiki/Human_behavior\n",
      "https://en.wikipedia.org/wiki/Motion_(physics)\n",
      "https://en.wikipedia.org/wiki/Physics\n",
      "https://en.wikipedia.org/wiki/Ancient_Greek\n",
      "https://en.wikipedia.org/wiki/Greek_language\n",
      "https://en.wikipedia.org/wiki/Modern_Greek\n",
      "https://en.wikipedia.org/wiki/Colloquialism\n",
      "https://en.wikipedia.org/wiki/Vernacular\n",
      "https://en.wikipedia.org/wiki/Dialect\n",
      "We've arrived at an article we've already seen, aborting search!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import urllib\n",
    "\n",
    "import bs4\n",
    "import requests\n",
    "\n",
    "\n",
    "start_url = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
    "target_url = \"https://en.wikipedia.org/wiki/Philosophy\"\n",
    "\n",
    "\n",
    "def find_first_link(url):\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # This div contains the article's body\n",
    "    # (June 2017 Note: Body nested in two div tags)\n",
    "    content_div = soup.find(\n",
    "        id=\"mw-content-text\").find(class_=\"mw-parser-output\")\n",
    "\n",
    "    # stores the first link found in the article, if the article contains no\n",
    "    # links this value will remain None\n",
    "    article_link = None\n",
    "\n",
    "    # Find all the direct children of content_div that are paragraphs\n",
    "    for element in content_div.find_all(\"p\", recursive=False):\n",
    "        # Find the first anchor tag that's a direct child of a paragraph.\n",
    "        # It's important to only look at direct children, because other types\n",
    "        # of link, e.g. footnotes and pronunciation, could come before the\n",
    "        # first link to an article. Those other link types aren't direct\n",
    "        # children though, they're in divs of various classes.\n",
    "        if element.find(\"a\", recursive=False):\n",
    "            article_link = element.find(\"a\", recursive=False).get('href')\n",
    "            break\n",
    "\n",
    "    if not article_link:\n",
    "        return\n",
    "\n",
    "    # Build a full url from the relative article_link url\n",
    "    first_link = urllib.parse.urljoin(\n",
    "        'https://en.wikipedia.org/', article_link)\n",
    "\n",
    "    return first_link\n",
    "\n",
    "\n",
    "def continue_crawl(search_history, target_url, max_steps=25):\n",
    "    if search_history[-1] == target_url:\n",
    "        print(\"We've found the target article!\")\n",
    "        return False\n",
    "    elif len(search_history) > max_steps:\n",
    "        print(\"The search has gone on suspiciously long, aborting search!\")\n",
    "        return False\n",
    "    elif search_history[-1] in search_history[:-1]:\n",
    "        print(\"We've arrived at an article we've already seen, aborting search!\")\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "article_chain = [start_url]\n",
    "\n",
    "while continue_crawl(article_chain, target_url):\n",
    "    print(article_chain[-1])\n",
    "\n",
    "    first_link = find_first_link(article_chain[-1])\n",
    "    if not first_link:\n",
    "        print(\"We've arrived at an article with no links, aborting search!\")\n",
    "        break\n",
    "\n",
    "    article_chain.append(first_link)\n",
    "\n",
    "    time.sleep(.25)  # Slow things down so as to not hammer Wikipedia's servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
